<center>
    <img src='res/MambaLRP_logo.jpeg', width='1000'>
</center>

This is the official implementation of the paper "[MambaLRP: Explaining Selective State Space Sequence Models]()".

## Requirements
Please make sure your setup satisfies the following requirements:
- Python==3.10.6
- Numpy
- PyTorch==2.1.1+cu118
- transformers==4.40.1
- pip install causal-conv1d>=1.2.0
- pip install mamba-ssm

## Acknowledgements
This repo is built using components from [Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/mamba) and [Mamba](https://github.com/state-spaces/mamba)
